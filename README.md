# Self-Critical (SCST) for Image Captioning in TensorFlow
Unofficial implementation of Self-Critical Sequence Training (SCST) and
various multi-head attention mechanisms.

## Description
This repo contains **experimental and unofficial** implementation of
image captioning frameworks including:

- Self-Critical Sequence Training (SCST) [[arxiv]](https://arxiv.org/abs/1612.00563)
    - Sampling is done via beam search [[arxiv]](https://arxiv.org/abs/1707.07998)
- Multi-Head Visual Attention
    - Additive (with optional Layer Norm) [[arxiv]](https://arxiv.org/abs/1903.01072)
    - Scaled Dot-Product [[arxiv]](https://arxiv.org/abs/1706.03762)

The features might not be completely tested. For a more stable implementation,
please refer to [this repo](https://github.com/jiahuei/COMIC-Compact-Image-Captioning-with-Attention).


## Dependencies
- tensorflow 1.9.0
- python 2.7
- java 1.8.0
- tqdm >= 4.24.0
- Pillow >= 3.1.2
- packaging >= 19.0
- requests >= 2.18.4


## Running the code
**More examples are given in `example.sh`.**

### First setup
Run `./src/setup.sh`. This will download the required Stanford models 
and run all the dataset pre-processing scripts.

### Training models
The training scheme is as follows:
1. Start with `decoder` mode (freezing the CNN)
1. Followed by `cnn_finetune` mode
1. Finally, `scst` mode

```bash
# MS-COCO
for mode in 'decoder' 'cnn_finetune' 'scst'
do
    python train.py  \
        --train_mode ${mode}  \
        --token_type 'word'  \
        --cnn_fm_projection 'tied'  \
        --attn_num_heads 8
done

# InstaPIC
for mode in 'decoder' 'cnn_finetune' 'scst'
do
    python train.py  \
        --train_mode ${mode}  \
        --dataset_file_pattern 'insta_{}_v25595_s15'  \
        --token_type 'word'  \
        --cnn_fm_projection 'independent'  \
        --attn_num_heads 8
done
```

### Inferencing
Just point `infer.py` to the directory containing the checkpoints. 
Model configurations are loaded from `config.pkl`.

```bash
# MS-COCO
python infer.py  \
	--infer_checkpoints_dir 'mscoco/word_add_softmax_h8_tie_lstm_run_01'

# InstaPIC
python infer.py  \
	--infer_checkpoints_dir 'insta/word_add_softmax_h8_ind_lstm_run_01'  \
	--annotations_file 'insta_testval_raw.json'
```

### List of arguments / flags
Please refer to 
[this repo](https://github.com/jiahuei/COMIC-Compact-Image-Captioning-with-Attention#main-arguments)


## Avoid re-downloading datasets
Re-downloading can be avoided by:
1. Editing `setup.sh`
2. Providing the path to the directory containing the dataset files

```bash
python coco_prepro.py --dataset_dir /path/to/coco/dataset
python insta_prepro.py --dataset_dir /path/to/insta/dataset
```

In the same way, both `train.py` and `infer.py` accept alternative dataset paths.

```bash
python train.py --dataset_dir /path/to/dataset
python infer.py --dataset_dir /path/to/dataset
```

This code assumes the following dataset directory structures:

### MS-COCO
```
{coco-folder}
+-- captions
|   +-- {folder and files generated by coco_prepro.py}
+-- test2014
|   +-- {image files}
+-- train2014
|   +-- {image files}
+-- val2014
    +-- {image files}
```

### InstaPIC-1.1M
```
{insta-folder}
+-- captions
|   +-- {folder and files generated by insta_prepro.py}
+-- images
|   +-- {image files}
+-- json
    +-- insta-caption-test1.json
    +-- insta-caption-train.json
```


## Project structure
```
.
+-- common
|   +-- {shared libraries and utility functions}
+-- datasets
|   +-- preprocessing
|       +-- {dataset pre-processing scripts}
+-- pretrained
|   +-- {pre-trained checkpoints for some COMIC models. Details are provided in a separate README.}
+-- src
    +-- {main scripts}
```


## Acknowledgements
Thanks to the developers of:
- [[coco-caption]](https://github.com/tylin/coco-caption/tree/3a9afb2682141a03e1cdc02b0df6770d2c884f6f)
- [[ruotianluo/self-critical.pytorch]](https://github.com/ruotianluo/self-critical.pytorch/tree/77dff3223ba2fefe26047ff6ef560c2aa0e1f942)
- [[ruotianluo/cider]](https://github.com/ruotianluo/cider/tree/dbb3960165d86202ed3c417b412a000fc8e717f3)
- [[weili-ict/SelfCriticalSequenceTraining-tensorflow]](https://github.com/weili-ict/SelfCriticalSequenceTraining-tensorflow/tree/cddf3f99bbd5b7ed96c12c6415fb6ae641d4816f)
- [[tensorflow]](https://github.com/tensorflow)


## License and Copyright
The project is open source under Apache-2.0 license (see the `LICENSE` file).


